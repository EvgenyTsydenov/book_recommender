{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every user has some preferences in relation to items offered by the service. It is possible to infer these preferences based on a variety of attributes (examples with books): book page view time, book clicks, adding to favorites, rating, writing reviews, and so on. \n",
    "\n",
    "All interactions between users and items can be converted to numbers and stored in so-called user-item interactions matrix. Here is an example from [this article](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada):\n",
    "\n",
    "![Interactions](pics/interactions.png \"Interactions image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of recommendation systems is to predict ratings which a user would give to items that he has not rated yet. In a simple way, the recommender algorithms can be classified as follows (the picture from the same [article](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)):\n",
    "\n",
    "![Recommender classification](pics/recommender_classification.png \"Recommender classification image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical difficulty in building recommender systems is the small number of ratings. Thus, to mitigate this problem, we will combine information from [Book–Crossing](book_crossing/data_prep) and [Goodreads](goodreads/data_prep) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import coo_matrix, find\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from models.gradient_descent import RecommenderGD\n",
    "from models.svd import RecommenderSVD\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set random seeds and supress warnings\n",
    "RANDOM_SEED = int(os.environ['RANDOM_SEED'])\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Book Crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed Book–Crossing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bc = os.path.join('book_crossing', 'data_prep')\n",
    "books_bc = pd.read_csv(os.path.join(path_bc, 'books.csv'),\n",
    "                       usecols=['isbn13', 'book_title'],\n",
    "                       index_col=['isbn13'],\n",
    "                       dtype={'isbn13': 'category',\n",
    "                              'book_title': 'str'})\n",
    "ratings_bc = pd.read_csv(os.path.join(path_bc, 'ratings.csv'),\n",
    "                         dtype={'user_id': 'category',\n",
    "                                'rating': 'uint8',\n",
    "                                'isbn13': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn13</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9780195153446</th>\n",
       "      <td>Classical Mythology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780002005012</th>\n",
       "      <td>Clara Callan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780060973124</th>\n",
       "      <td>Decision in Normandy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         book_title\n",
       "isbn13                             \n",
       "9780195153446   Classical Mythology\n",
       "9780002005012          Clara Callan\n",
       "9780060973124  Decision in Normandy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_bc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>isbn13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>5</td>\n",
       "      <td>9780155061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>3</td>\n",
       "      <td>9780521656153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>6</td>\n",
       "      <td>9780521795029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  rating         isbn13\n",
       "1  276726       5  9780155061224\n",
       "3  276729       3  9780521656153\n",
       "4  276729       6  9780521795029"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all implicit ratings\n",
    "ratings_bc = ratings_bc[ratings_bc['rating'] > 0]\n",
    "ratings_bc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books: 270947\n",
      "Number of ratings: 384127\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of books: {len(books_bc)}')\n",
    "print(f'Number of ratings: {len(ratings_bc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn13</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9780195153446</th>\n",
       "      <td>Classical Mythology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780002005012</th>\n",
       "      <td>Clara Callan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title\n",
       "isbn13                            \n",
       "9780195153446  Classical Mythology\n",
       "9780002005012         Clara Callan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_bc.rename(columns={'book_title': 'title'},\n",
    "                inplace=True)\n",
    "books_bc.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Goodreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed Goodreads data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gr = os.path.join('goodreads', 'data_prep')\n",
    "books_gr = pd.read_csv(os.path.join(path_gr, 'books.csv'),\n",
    "                       usecols=['isbn13', 'title', 'work_id', 'book_id'],\n",
    "                       index_col=['book_id'],\n",
    "                       dtype={'isbn13': 'category', 'work_id': 'category',\n",
    "                              'title': 'str', 'book_id': 'category'})\n",
    "ratings_gr = pd.read_csv(os.path.join(path_gr, 'ratings.csv'),\n",
    "                         usecols=['user_id', 'book_id', 'rating'],\n",
    "                         dtype={'user_id': 'category',\n",
    "                                'rating': 'float16',\n",
    "                                'book_id': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5333265</th>\n",
       "      <td>9780312853129</td>\n",
       "      <td>5400751</td>\n",
       "      <td>W.C. Fields: A Life on Film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333909</th>\n",
       "      <td>9780743509985</td>\n",
       "      <td>1323437</td>\n",
       "      <td>Good Harbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066819</th>\n",
       "      <td>9780743294294</td>\n",
       "      <td>6243154</td>\n",
       "      <td>Best Friends Forever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                isbn13  work_id                        title\n",
       "book_id                                                     \n",
       "5333265  9780312853129  5400751  W.C. Fields: A Life on Film\n",
       "1333909  9780743509985  1323437                  Good Harbor\n",
       "6066819  9780743294294  6243154         Best Friends Forever"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_gr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating book_id                           user_id\n",
       "0       5      12  8842281e1d1347389f2ab93d60773d4d\n",
       "1       5      21  8842281e1d1347389f2ab93d60773d4d\n",
       "2       5      30  8842281e1d1347389f2ab93d60773d4d"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all implicit ratings\n",
    "ratings_gr = ratings_gr[~ratings_gr['rating'].isna()]\n",
    "ratings_gr['rating'] = ratings_gr['rating'].astype('uint8')\n",
    "ratings_gr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books: 1599130\n",
      "Number of ratings: 89625534\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of books: {len(books_gr)}')\n",
    "print(f'Number of ratings: {len(ratings_gr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember from the data preprocessing stage, there are some duplicated ISBNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26812295</th>\n",
       "      <td>9780007255764</td>\n",
       "      <td>19269242</td>\n",
       "      <td>The Year of Reading Dangerously: How Fifty Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25412569</th>\n",
       "      <td>9780007255764</td>\n",
       "      <td>19269242</td>\n",
       "      <td>The Year of Reading Dangerously: How Fifty Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25401812</th>\n",
       "      <td>9780007282586</td>\n",
       "      <td>2288775</td>\n",
       "      <td>A Murder Is Announced (Miss Marple, #5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25386782</th>\n",
       "      <td>9780007282586</td>\n",
       "      <td>2288775</td>\n",
       "      <td>A Murder Is Announced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13146527</th>\n",
       "      <td>9780007395200</td>\n",
       "      <td>2677305</td>\n",
       "      <td>Number the Stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 isbn13   work_id  \\\n",
       "book_id                             \n",
       "26812295  9780007255764  19269242   \n",
       "25412569  9780007255764  19269242   \n",
       "25401812  9780007282586   2288775   \n",
       "25386782  9780007282586   2288775   \n",
       "13146527  9780007395200   2677305   \n",
       "\n",
       "                                                      title  \n",
       "book_id                                                      \n",
       "26812295  The Year of Reading Dangerously: How Fifty Gre...  \n",
       "25412569  The Year of Reading Dangerously: How Fifty Gre...  \n",
       "25401812            A Murder Is Announced (Miss Marple, #5)  \n",
       "25386782                              A Murder Is Announced  \n",
       "13146527                                   Number the Stars  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_gr_duplicates = books_gr[books_gr.duplicated(['isbn13'], keep=False)]\\\n",
    "    .sort_values('isbn13')\n",
    "books_gr_duplicates.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to delete the duplicates and made the corresponding changes in the rating dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group them and get indexes\n",
    "books_gr_duplicates_idx = books_gr_duplicates\\\n",
    "    .groupby(['isbn13'], observed=True)\\\n",
    "    .apply(lambda x: list(x.index)).tolist()\n",
    "\n",
    "# Iterate over each group and keep only one book id\n",
    "to_replace = {}\n",
    "for book_group in books_gr_duplicates_idx:\n",
    "    to_leave = book_group.pop()\n",
    "    for index in book_group:\n",
    "        to_replace[index] = to_leave\n",
    "\n",
    "# Drop duplicates from the book dataset\n",
    "books_gr.drop(index=to_replace.keys(), inplace=True)\n",
    "\n",
    "# Replace in the ratings\n",
    "ratings_gr['book_id'] = ratings_gr['book_id']\\\n",
    "    .map(lambda x: to_replace.get(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation causes duplicated rows in ratings, so we will drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_gr.drop_duplicates(['user_id', 'book_id'],\n",
    "                           keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `book_id` to `isbn13`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>9780517226957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>9780767908184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                           user_id         isbn13\n",
       "0       5  8842281e1d1347389f2ab93d60773d4d  9780517226957\n",
       "1       5  8842281e1d1347389f2ab93d60773d4d  9780767908184"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_gr = ratings_gr.merge(books_gr[['isbn13']], left_on='book_id',\n",
    "                              right_index=True, how='left')\n",
    "ratings_gr.drop(columns=['book_id'], inplace=True)\n",
    "ratings_gr.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine book info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books_gr.merge(books_bc, left_on='isbn13',\n",
    "                       right_index=True, how='outer')\n",
    "\n",
    "# Merge titles\n",
    "books['title'] = books['title_x']\n",
    "missing_titles = books['title'].isna()\n",
    "books.loc[missing_titles, 'title'] = books.loc[missing_titles, 'title_y']\n",
    "\n",
    "# Drop unused columns\n",
    "books.drop(columns=['title_x', 'title_y'], inplace=True)\n",
    "\n",
    "# Set ISBN as index\n",
    "books.set_index('isbn13', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since books from Book-Crossing dataset have no work ids, we need to create unique ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isbn13</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9780312853129</th>\n",
       "      <td>5400751</td>\n",
       "      <td>W.C. Fields: A Life on Film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780743509985</th>\n",
       "      <td>1323437</td>\n",
       "      <td>Good Harbor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               work_id                        title\n",
       "isbn13                                             \n",
       "9780312853129  5400751  W.C. Fields: A Life on Film\n",
       "9780743509985  1323437                  Good Harbor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['work_id'] = books['work_id'].astype('float')\n",
    "missing_work_ids = books['work_id'].isna()\n",
    "existing_work_ids = set(books.loc[~missing_work_ids, 'work_id'])\n",
    "\n",
    "new_work_ids = []\n",
    "counter = 0\n",
    "nans_count = missing_work_ids.sum()\n",
    "while len(new_work_ids) < nans_count:\n",
    "    if counter not in existing_work_ids:\n",
    "        new_work_ids.append(counter)\n",
    "    counter += 1\n",
    "\n",
    "books.loc[missing_work_ids, 'work_id'] = np.array(new_work_ids)\n",
    "books['work_id'] = books['work_id'].astype('int').astype('category')\n",
    "books.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging the rating data, we need to scale ratings to the common range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale goodreads ratings to the range from 1 to 10\n",
    "ratings_gr['rating'] *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that users from Goodreads and Book-Crossing communities are completely different. Check the ids to be unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_gr['user_id'].isin(ratings_bc['user_id']).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append Book-Crossing ratings to Goodreads ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>work_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>9780517226957</td>\n",
       "      <td>135328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>9780767908184</td>\n",
       "      <td>2305997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                           user_id         isbn13  work_id\n",
       "0      10  8842281e1d1347389f2ab93d60773d4d  9780517226957   135328\n",
       "1      10  8842281e1d1347389f2ab93d60773d4d  9780767908184  2305997"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = ratings_gr.append(ratings_bc)\n",
    "\n",
    "# Add work_ids\n",
    "ratings = ratings.merge(books[['work_id']], how='left',\n",
    "                        left_on='isbn13', right_index=True)\n",
    "ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since people most often rate the book content rather than a particular edition of the book, we will build recommendations using `work_id` instead of `isbn13`.\n",
    "\n",
    "Some users may have rated different editions of the same book. Let's average their ratings so that there is only one rating per book from each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>135328</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>2305997</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>89369</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>1699340</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  work_id  rating\n",
       "0  8842281e1d1347389f2ab93d60773d4d   135328    10.0\n",
       "1  8842281e1d1347389f2ab93d60773d4d  2305997    10.0\n",
       "2  8842281e1d1347389f2ab93d60773d4d    89369    10.0\n",
       "3  8842281e1d1347389f2ab93d60773d4d  1699340    10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_per_work = ratings[['work_id', 'user_id', 'rating']]\\\n",
    "    .groupby(['work_id', 'user_id'], observed=True).mean()\n",
    "\n",
    "# Drop duplicated ratings\n",
    "work_ratings = ratings.drop_duplicates(['user_id', 'work_id'], keep='first')\n",
    "work_ratings = work_ratings[['user_id', 'work_id']]\\\n",
    "    .merge(ratings_per_work, left_on=['user_id', 'work_id'],\n",
    "           right_on=['user_id', 'work_id'], how='left')\n",
    "work_ratings.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of work_id which have ratings: 1141892\n",
      "Number of work_id which have at least 5 ratings: 568805\n",
      "Number of work_id which have at least 10 ratings: 394932\n",
      "\n",
      "Number of user_id which have ratings: 876176\n",
      "Number of user_id which have at least 5 ratings: 750186\n",
      "Number of user_id which have at least 10 ratings: 692320\n",
      "\n",
      "Total number of ratings: 89686460\n"
     ]
    }
   ],
   "source": [
    "# Group data to get statistics\n",
    "for parameter in ['work_id', 'user_id']:\n",
    "    ratings_by_param = work_ratings[[parameter, 'rating']]\\\n",
    "        .groupby(parameter, observed=True).count()\n",
    "    rated_count = len(ratings_by_param)\n",
    "    five_times_rated_count = (ratings_by_param['rating'] >= 5).sum()\n",
    "    ten_times_rated_count = (ratings_by_param['rating'] >= 10).sum()\n",
    "    print(f'Number of {parameter} which have ratings: {rated_count}')\n",
    "    print(f'Number of {parameter} which have at least 5 ratings: '\n",
    "          f'{five_times_rated_count}')\n",
    "    print(f'Number of {parameter} which have at least 10 ratings: '\n",
    "          f'{ten_times_rated_count}\\n')\n",
    "print(f'Total number of ratings: {len(work_ratings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the recommender performance, we need to split data into train, validation, and test datasets. We will use information about users who left at least 10 ratings (60% for training, 20% for testing and validation). Since the dataset does not contain rating timestamps, we will split the data in a random way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who left at least ten ratings: 692320 (79.02%)\n"
     ]
    }
   ],
   "source": [
    "ratings_per_user = work_ratings.groupby('user_id', observed=True)\n",
    "ratings_per_user_count = ratings_per_user['rating'].count()\n",
    "min_ten_users = ratings_per_user_count[ratings_per_user_count >= 10].index\n",
    "print(f'Number of users who left at least ten ratings: '\n",
    "      f'{len(min_ten_users)} '\n",
    "      f'({round(len(min_ten_users) * 100 / len(ratings_per_user), 2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop users with a little number of ratings\n",
    "work_ratings_min_ten = ratings_per_user.filter(lambda x: len(x) >= 10)\n",
    "\n",
    "# Drop unused books\n",
    "books = books[books['work_id'].isin(work_ratings_min_ten['work_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53439522, 3), (17813175, 3), (17813174, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train, test, and val datasets\n",
    "work_ratings_train, work_ratings_val = train_test_split(\n",
    "    work_ratings_min_ten, test_size=0.4, random_state=RANDOM_SEED,\n",
    "    stratify=work_ratings_min_ten['user_id'])\n",
    "work_ratings_test, work_ratings_val = train_test_split(\n",
    "    work_ratings_val, test_size=0.5, random_state=RANDOM_SEED,\n",
    "    stratify=work_ratings_val['user_id'])\n",
    "\n",
    "# Show the shape\n",
    "work_ratings_train.shape, work_ratings_val.shape, work_ratings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting, we can see that some books from test and validation data are absent from the training one. We cannot test performance of the algorithms on these samples. Thus, we have to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53439522, 3), (17724293, 3), (17723743, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_ratings_val = work_ratings_val[work_ratings_val['work_id']\\\n",
    "    .isin(work_ratings_train['work_id'])]\n",
    "work_ratings_test = work_ratings_test[work_ratings_test['work_id']\\\n",
    "    .isin(work_ratings_train['work_id'])]\n",
    "\n",
    "# Show the shape\n",
    "work_ratings_train.shape, work_ratings_val.shape, work_ratings_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare multiple users' scores, we need to normalize them. Because one user may always give high ratings (8 to 10), while another may give all ratings from 6 to 8. However, the tastes of these users may be exactly the same. Thus, we will use StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_user_scalers(ratings: pd.DataFrame) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"Get statistics per suer to normalize their ratings.\n",
    "\n",
    "    :param ratings: all ratings.\n",
    "    :return: average and standard deviation of users' ratings.\n",
    "    \"\"\"\n",
    "    user_ratings_scalers = {}\n",
    "    scaler = StandardScaler()\n",
    "    for user_id, user_data in ratings.groupby('user_id'):\n",
    "        scaler.fit(user_data[['rating']])\n",
    "        user_ratings_scalers[user_id] = scaler.scale_, scaler.mean_\n",
    "    return user_ratings_scalers\n",
    "\n",
    "\n",
    "def ratings_normalize(user_ratings: pd.Series, user_scale: float,\n",
    "                      user_mean: float) -> pd.Series:\n",
    "    \"\"\"Transform user's ratings.\n",
    "\n",
    "    :param user_ratings: user's ratings.\n",
    "    :param user_scale: standard deviation of user's ratings.\n",
    "    :param user_mean: average of user's ratings.\n",
    "    :return: transformed values.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.scale_ = user_scale\n",
    "    scaler.mean_ = user_mean\n",
    "    return scaler.transform(user_ratings.values.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be done only for training dataset\n",
    "user_ratings_scalers = fit_user_scalers(work_ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all datasets\n",
    "for dataset in [work_ratings_val, work_ratings_test, work_ratings_train]:\n",
    "    dataset['rating_scaled'] = dataset.groupby('user_id')['rating']\\\n",
    "        .transform(lambda x: ratings_normalize(x, *user_ratings_scalers[x.name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save preprocessed data to use it in training scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler info\n",
    "with open(os.path.join('data', 'user_ratings_scalers.pkl'), 'wb') as file:\n",
    "    pickle.dump(user_ratings_scalers, file)\n",
    "\n",
    "# Save datasets\n",
    "work_ratings_train.to_csv(os.path.join('data', 'work_ratings_train.csv'),\n",
    "                          index=False)\n",
    "work_ratings_val.to_csv(os.path.join('data', 'work_ratings_val.csv'),\n",
    "                        index=False)\n",
    "work_ratings_test.to_csv(os.path.join('data', 'work_ratings_test.csv'),\n",
    "                         index=False)\n",
    "books.to_csv(os.path.join('data', 'books.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the previously saved data in order to use in this notebook the same data loading pipeline as in the training scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler info\n",
    "with open(os.path.join('data', 'user_ratings_scalers.pkl'), 'rb') as file:\n",
    "    user_ratings_scalers = pickle.load(file)\n",
    "\n",
    "# Load datasets\n",
    "work_ratings_train = pd.read_csv(os.path.join('data', 'work_ratings_train.csv'),\n",
    "                                 dtype={'rating_scaled': 'float16', \n",
    "                                        'rating': 'float16'})\n",
    "work_ratings_val = pd.read_csv(os.path.join('data', 'work_ratings_val.csv'),\n",
    "                               dtype={'rating_scaled': 'float16', \n",
    "                                      'rating': 'float16'})\n",
    "work_ratings_test = pd.read_csv(os.path.join('data', 'work_ratings_test.csv'),\n",
    "                                dtype={'rating_scaled': 'float16', \n",
    "                                       'rating': 'float16'})\n",
    "books = pd.read_csv(os.path.join('data', 'books.csv'), index_col='isbn13')\n",
    "\n",
    "# Extract categories\n",
    "work_ratings_train['user_id'] = work_ratings_train['user_id'].astype('category')\n",
    "work_ratings_train['work_id'] = work_ratings_train['work_id'].astype('category')\n",
    "user_categories = CategoricalDtype(\n",
    "    categories=work_ratings_train['user_id'].cat.categories)\n",
    "item_categories = CategoricalDtype(\n",
    "    categories=work_ratings_train['work_id'].cat.categories)\n",
    "\n",
    "# Convert values to categories from train data\n",
    "work_ratings_val['user_id'] = work_ratings_val['user_id'].astype(user_categories)\n",
    "work_ratings_test['user_id'] = work_ratings_test['user_id'].astype(user_categories)\n",
    "work_ratings_val['work_id'] = work_ratings_val['work_id'].astype(item_categories)\n",
    "work_ratings_test['work_id'] = work_ratings_test['work_id'].astype(item_categories)\n",
    "\n",
    "# Drop unknown users or items\n",
    "mask = work_ratings_val['user_id'].isna() | work_ratings_val['work_id'].isna()\n",
    "work_ratings_val = work_ratings_val[~mask]\n",
    "\n",
    "# Create interaction matrix\n",
    "interactions_train = coo_matrix((work_ratings_train['rating_scaled'],\n",
    "                                 (work_ratings_train['user_id'].cat.codes,\n",
    "                                  work_ratings_train['work_id'].cat.codes)))\n",
    "interactions_val = coo_matrix((work_ratings_val['rating_scaled'],\n",
    "                               (work_ratings_val['user_id'].cat.codes,\n",
    "                                work_ratings_val['work_id'].cat.codes)))\n",
    "interactions_test = coo_matrix((work_ratings_test['rating_scaled'],\n",
    "                                (work_ratings_test['user_id'].cat.codes,\n",
    "                                 work_ratings_test['work_id'].cat.codes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of recommendation can be obtained only with a certain level of knowledge about user's preferences for items. Collaborative filtering can be implemented using memory-based algorithms and model-based algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-based approach uses data mining algorithms or machine learning techniques. These methods predict the rating that the user would give to a book. The advantage of the approach is the possibility to reduce the dimensionality of the feature space. Thus, there is no need for resource-intensive computations with sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try a model-based matrix factorization algorithm which is based on the idea that the matrix with user ratings `M` can be decomposed into two components. The first matrix `X` will describe user's features, and the second one — `Y` will describe the features of the books:\n",
    "\n",
    "$$ M \\approx X \\times Y^T $$\n",
    "\n",
    "Then the task of training the model is to determine matrices `X` and `Y` so that the difference between the predicted and real ratings was minimal: \n",
    "\n",
    "$$\\min_{X \\in \\mathbb R^{m \\times d}, Y \\in \\mathbb R^{n \\times d}} \\sum_{i, j} (M_{ij} - \\langle X_{i}, Y_{j} \\rangle)^2$$\n",
    "\n",
    "where  `m` – number of users, `n` – number of books, `d` – size of book and user embedding. Details can be found [here](https://developers.google.com/machine-learning/recommendation/collaborative/basics). The following image [from here](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada) presents the idea:\n",
    "\n",
    "![Matrix factorization](pics/matrix_factorization.png \"Matrix factorization image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach we will try is SVD (Singular Value Decomposition) that decomposes the interaction matrix into three matrices:\n",
    "\n",
    "$$ M = X \\times \\Sigma \\times Y^T $$\n",
    "\n",
    "where $\\Sigma$ — weights diagonal. The dimension of the latent space is a hyperparameter that should be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding model was implemented with [scikit-learn](https://scikit-learn.org/stable/). The code can be found [here](models/svd.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the train data: 0.9777280039497352\n",
      "RMSE on the validation data: 1.0323204293967774\n"
     ]
    }
   ],
   "source": [
    "# Train a baseline model\n",
    "svd_baseline = RecommenderSVD(RANDOM_SEED)\n",
    "svd_baseline.fit(interactions_train, embed_size=10)\n",
    "\n",
    "# Extract true values\n",
    "user_ids_train, item_ids_train, ratings_train_true = find(interactions_train)\n",
    "user_ids_val, item_ids_val, ratings_val_true = find(interactions_val)\n",
    "ratings_train_true = ratings_train_true.reshape(-1, 1)\n",
    "ratings_val_true = ratings_val_true.reshape(-1, 1)\n",
    "\n",
    "# Predict \n",
    "ratings_train_predicts = svd_baseline.predict((user_ids_train, item_ids_train))\n",
    "ratings_val_predicts = svd_baseline.predict((user_ids_val, item_ids_val))\n",
    "\n",
    "# Evaluate\n",
    "print(f'RMSE on the train data: '\n",
    "      f'{mse(ratings_train_true, ratings_train_predicts, squared=False)}')\n",
    "print(f'RMSE on the validation data: '\n",
    "      f'{mse(ratings_val_true, ratings_val_predicts, squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "svd_model_path = os.path.join('trained_models', 'svd_baseline.pkl')\n",
    "with open(svd_model_path, 'wb') as model_file:\n",
    "    pickle.dump(svd_baseline, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the train data with random embeddings: 2.778924792514173\n",
      "RMSE on the validation data with random embeddings: 2.7988221171322616\n"
     ]
    }
   ],
   "source": [
    "# Check if the baseline model works better than random embeddings\n",
    "svd_random = RecommenderSVD()\n",
    "svd_random._user_embeds = np.random.random(size=svd_baseline._user_embeds.shape)\n",
    "svd_random._item_embeds = np.random.random(size=svd_baseline._item_embeds.shape)\n",
    "\n",
    "# Predict \n",
    "random_train_predicts = svd_random.predict((user_ids_train, item_ids_train))\n",
    "random_val_predicts = svd_random.predict((user_ids_val, item_ids_val))\n",
    "\n",
    "# Evaluate\n",
    "print(f'RMSE on the train data with random embeddings: '\n",
    "      f'{mse(ratings_train_true, random_train_predicts, squared=False)}')\n",
    "print(f'RMSE on the validation data with random embeddings: '\n",
    "      f'{mse(ratings_val_true, random_val_predicts, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is better. Hope, we are on the right track.\n",
    "\n",
    "Next, we will tune the embedding size. Since the calculation process long enough, it was executed from this [script](training_scripts/svd.py) and monitored via [neptune.ai](https://neptune.ai/). Metrics for different embedding size are shown below and can be viewed [in the neptune dashboard](https://app.neptune.ai/evgenytsydenov/book-recommender/experiments?split=tbl&dash=charts&viewId=5f4a8005-572a-4055-aaa2-8f825b31ef8d):\n",
    "\n",
    "![SVD tuning](pics/svd_tuning.png \"SVD tuning image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing of embedding size, the SVD model requires much more RAM and get better results on the training data but does not lead to significant decreasing of validation error. Thus, large embeddings cause overfitting.\n",
    "\n",
    "In the data we used, there are far more unknown ratings than there are known ones. Thus, the interaction matrix contains a lot of zeros. Since we scaled ratings in the preprocessing stage, zeros in our datasets mean average ratings of each user. This is a good baseline. However, it can be improved if to use user and item biases. More details about this can be found in the book [\"Practical Recommender Systems\" by Kim Falk](https://www.oreilly.com/library/view/practical-recommender-systems/9781617292705/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the difference between predicted and real values of ratings in the equation above, we can use the gradient descent. The method consists in the following steps:\n",
    "\n",
    "1) Random initialization of user and item embeddings.\n",
    "2) Compute dot product of embeddings for known user and book pair.\n",
    "3) Compare predicted value with the real one.\n",
    "4) Update embeddings to minimize the difference.\n",
    "5) Repeat steps 2–4.\n",
    "\n",
    "The corresponding model was implemented with [TensorFlow](https://www.tensorflow.org/). The code can be found [here](models/gradient_descent.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the global Keras state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build a baseline model in GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    gd_baseline = RecommenderGD(users_count=len(user_categories.categories),\n",
    "                                books_count=len(item_categories.categories),\n",
    "                                embed_size=10, random_seed=RANDOM_SEED)\n",
    "\n",
    "# Save its structure\n",
    "tf.keras.utils.plot_model(gd_baseline.build_graph(), rankdir='LR',\n",
    "                          to_file='pics/recommender_gd.png',\n",
    "                          show_shapes=True, show_layer_names=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GD model structure](pics/recommender_gd.png \"GD model structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6524/6524 [==============================] - 50s 7ms/step - loss: 0.3960 - root_mean_squared_error: 0.9720 - val_loss: 0.3964 - val_root_mean_squared_error: 0.9929\n",
      "INFO:tensorflow:Assets written to: trained_models/gd_baseline/assets\n",
      "Epoch 2/6\n",
      "6524/6524 [==============================] - 46s 7ms/step - loss: 0.3701 - root_mean_squared_error: 0.9396 - val_loss: 0.3847 - val_root_mean_squared_error: 0.9785\n",
      "INFO:tensorflow:Assets written to: trained_models/gd_baseline/assets\n",
      "Epoch 3/6\n",
      "6524/6524 [==============================] - 45s 7ms/step - loss: 0.3533 - root_mean_squared_error: 0.9173 - val_loss: 0.3789 - val_root_mean_squared_error: 0.9714\n",
      "INFO:tensorflow:Assets written to: trained_models/gd_baseline/assets\n",
      "Epoch 4/6\n",
      "6524/6524 [==============================] - 45s 7ms/step - loss: 0.3396 - root_mean_squared_error: 0.8986 - val_loss: 0.3775 - val_root_mean_squared_error: 0.9701\n",
      "INFO:tensorflow:Assets written to: trained_models/gd_baseline/assets\n",
      "Epoch 5/6\n",
      "6524/6524 [==============================] - 45s 7ms/step - loss: 0.3293 - root_mean_squared_error: 0.8843 - val_loss: 0.3780 - val_root_mean_squared_error: 0.9714\n",
      "Epoch 6/6\n",
      "6524/6524 [==============================] - 45s 7ms/step - loss: 0.3213 - root_mean_squared_error: 0.8732 - val_loss: 0.3794 - val_root_mean_squared_error: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# To save the best weights\n",
    "gd_model_path = os.path.join('trained_models', 'gd_baseline')\n",
    "model_checkpoint = ModelCheckpoint(filepath=gd_model_path,      \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_obnly=True)\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):\n",
    "    gd_baseline.compile(loss=tf.keras.losses.Huber(),\n",
    "                        optimizer=tf.keras.optimizers.Adam(),\n",
    "                        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    gd_baseline.fit(x=[work_ratings_train['user_id'].cat.codes.values,\n",
    "                       work_ratings_train['work_id'].cat.codes.values],\n",
    "                    y=work_ratings_train['rating_scaled'].values,\n",
    "                    validation_data=([work_ratings_val['user_id'].cat.codes.values,\n",
    "                                      work_ratings_val['work_id'].cat.codes.values],\n",
    "                                     work_ratings_val['rating_scaled'].values),\n",
    "                    callbacks=[model_checkpoint],\n",
    "                    batch_size=8192, epochs=6, verbose=1);\n",
    "    \n",
    "    # Load the best weights\n",
    "    gd_baseline.load_weights(gd_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6524/6524 [==============================] - 9s 1ms/step - loss: 0.3222 - root_mean_squared_error: 0.8738\n",
      "2164/2164 [==============================] - 3s 1ms/step - loss: 0.3775 - root_mean_squared_error: 0.9701\n",
      "RMSE on the train data: 0.8738026022911072\n",
      "RMSE on the validation data: 0.970125138759613\n"
     ]
    }
   ],
   "source": [
    "# Evaluate \n",
    "train_metrics = gd_baseline.evaluate((work_ratings_train['user_id'].cat.codes,\n",
    "                                      work_ratings_train['work_id'].cat.codes),\n",
    "                                     work_ratings_train[['rating_scaled']],\n",
    "                                     return_dict=True, batch_size=8192)\n",
    "val_metrics = gd_baseline.evaluate((work_ratings_val['user_id'].cat.codes,\n",
    "                                    work_ratings_val['work_id'].cat.codes),\n",
    "                                   work_ratings_val[['rating_scaled']],\n",
    "                                   return_dict=True, batch_size=8192)\n",
    "\n",
    "print(f'RMSE on the train data: {train_metrics[\"root_mean_squared_error\"]}')\n",
    "print(f'RMSE on the validation data: {val_metrics[\"root_mean_squared_error\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are better than from the SVD baseline. However, this approach also suffers from fast overfitting, so it demands regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}